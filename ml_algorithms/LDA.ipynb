{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "* [Probabilistic Topic Models - Surveying a suite of algorithms that offer a solution to managing large document archives -  David M. Blei](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)\n",
    "    * Review article giving high level overview\n",
    "* [Probabilistic Topic Models - Mark Steyvers, Tom Griffiths](http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf)\n",
    "    * Describes Gibbs sampling approach\n",
    "* [Finding scientific topics - Thomas L. Griffiths,Mark Steyvers](http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf)\n",
    "    * Describes Gibbs sampling approach\n",
    "* [Gibbs Sampler for LDA](https://wiseodd.github.io/techblog/2017/09/07/lda-gibbs/)\n",
    "    * [code](https://github.com/wiseodd/probabilistic-models/tree/master/models/bayesian)\n",
    "* [Parameter estimation for text analysis](http://www.arbylon.net/publications/text-est.pdf)\n",
    "* Machine Learning: Clustering & Retrieval ([Coursera course](https://www.coursera.org/learn/ml-clustering-and-retrieval/home/welcome)). Week 5: Mixed Membership Modeling via Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Topic Models\n",
    "\n",
    "* Documents are mixtures of topics. Each document has a distribution $P(z)$ over topics $z$.\n",
    "* A topic is a probability distribution over words. The conditional distribution for a word $w$ given topic $z$ is denoted as $P(w \\mid z)$.\n",
    "\n",
    "This means that the distribution of word $w$ within a document can be written as:\n",
    "\n",
    "$$\n",
    "P(w) = \\sum^{K}_{k=1} P(w \\mid z = k) P(z = k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative process\n",
    "\n",
    "Given:\n",
    "* $\\alpha$ the prior on the Dirichtlet distribution modelling the document topic distributions. $\\alpha$ is a vector of size $K$ the number of topics.\n",
    "* $\\beta$ the prior on the Diricthlet distribution modelling the topic word distribution. $\\beta$ is a vector of size $W$ the vocabulary of possible words.\n",
    "\n",
    "We can generate bag-of-word document samples by:\n",
    "\n",
    "1. For each document $i \\in \\{1,\\ldots, D \\}$ sample the topic distribution $\\theta_{i}\\sim \\operatorname{Dir}(\\alpha)$\n",
    "2. For each topic $k \\in \\{1, \\ldots, K \\}$ sample the word distribution $\\varphi_{k}\\sim \\operatorname {Dir}(\\beta)$\n",
    "3. Generate the words in a two-stage process: For each document $i$ and each word $j$ from that document:\n",
    "    1. Sample a topic $z_{i,j}\\sim \\operatorname{Categorical} (\\theta_{i})$\n",
    "    2. Sample a word from that topic $w_{i,j} \\sim \\operatorname{Categorical} (\\varphi_{z_{i,j}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_topics = 2\n",
    "n_words_vocab = 5\n",
    "n_documents = 20\n",
    "\n",
    "# Topic distribution ùúÉ for each document\n",
    "doc_topic_dist = np.zeros((n_documents, n_topics))\n",
    "doc_topic_dist[:n_documents//2, 0] = 1\n",
    "doc_topic_dist[n_documents//2:, 1] = 1\n",
    "# Word distribution ùúë for each topic\n",
    "topic_word_dist = np.array([\n",
    "    [1/3, 1/3, 1/3,  0.,  0.],\n",
    "    [0.,  0.,  1/3,  1/3, 1/3]])\n",
    "\n",
    "# Generate sample of documents\n",
    "documents_words = []  # Words in documents\n",
    "documents_word_topics = []  # Topic assignment for words in document\n",
    "for doc_idx in range(n_documents):\n",
    "    # Sample number of words in document by Poisson distribution\n",
    "    n_words_doc = int(1 + np.random.poisson(lam=4., size=1))\n",
    "    doc_topics = np.random.choice(\n",
    "        a=n_topics, p=doc_topic_dist[doc_idx,:], size=n_words_doc)\n",
    "    doc_words = [\n",
    "        np.random.choice(\n",
    "            a=n_words_vocab, \n",
    "            p=topic_word_dist[doc_topics[word_idx], :])\n",
    "        for word_idx in range(n_words_doc)]\n",
    "    documents_word_topics.append(doc_topics.tolist())\n",
    "    documents_words.append(doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_assignment(\n",
    "        doc_word_assignment, \n",
    "        doc_word_topic_assignment,\n",
    "        title):\n",
    "    max_doc_len = max(map(len, doc_word_assignment))\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "    for doc_idx in range(n_documents):\n",
    "        word_plotted_count = np.zeros((n_words_vocab, ))\n",
    "        for doc_word_idx, word in enumerate(doc_word_assignment[doc_idx]):\n",
    "            topic_idx = doc_word_topic_assignment[doc_idx][doc_word_idx]\n",
    "            shift = word_plotted_count[word] / max_doc_len\n",
    "            if topic_idx == 0:\n",
    "                ax.scatter(\n",
    "                    [word + shift], [doc_idx], \n",
    "                    marker='o', c='w', edgecolors='k', label='topic A')\n",
    "            if topic_idx == 1:\n",
    "                ax.scatter(\n",
    "                    [word + shift], [doc_idx], \n",
    "                    marker='o', c='k', edgecolors='k', label='topic B')\n",
    "            word_plotted_count[word] += 1\n",
    "    ax.set_yticks(np.arange(n_documents))\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_xticklabels(('River', 'Stream', 'Bank', 'Money', 'Loan'))\n",
    "    ax.set_ylabel('Sets')\n",
    "    ax.set_xlabel('Objects')\n",
    "    ax.set_title(title)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_topic_assignment(documents_words, documents_word_topics, 'Document topic assignement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs sampling\n",
    "\n",
    "Gibbs sampling will iteratively randomly sample topic assignments to each word in the corpus.\n",
    "\n",
    "The probability of assigning word $w$ to topic $z = k$ is:\n",
    "\n",
    "$$\n",
    "P(z = k \\mid w) = \\frac{P(w \\mid z=k) P(z=k)}{P(w)}\n",
    "$$\n",
    "\n",
    "Which for each document $i$ word $w_{i,j}$ and topic $k$ becomes:\n",
    "$$\n",
    "P(z_i = k \\mid w_{i,j})  = \\frac{\\theta_{i,k} \\varphi_{w_{i,j},k}}{\\sum^{K}_{t=1} \\theta_{i,t} \\varphi_{w_{i,j},t}}\n",
    "$$\n",
    "\n",
    "In regular Gibbs sampling the following steps are iteratively executed:\n",
    "1. Repeat for all documents $i \\in \\{1,\\ldots, D \\}$:\n",
    "    1. Randomly reassign topic assignments $z_{i,j}$ by sampling from $P(z \\mid w)$.\n",
    "    2. Randomly reassign document topic proportions $\\theta_i$ based on all topic assignements $\\mathbf{z}_i$ in document $i$.\n",
    "2. Randomly reassign word distributions for each topic $\\varphi_{k}$ based on the assignements in the entire corpus.\n",
    "\n",
    "\n",
    "### Collapsed Gibbs sampling\n",
    "\n",
    "We can actually marginalise over all the uncertainty of our model parameters and just sample the word-topic assignment variables $z_{i,j}$ and avoid the need to sample the topic vocabulary distributions $\\varphi_{k}$. We can \"collapse\" all these model parameters $\\theta$ and $\\varphi$.\n",
    "\n",
    "\n",
    "The Collapsed Gibbs sampling procedure considers each word token in the text collection in turn, and estimates the probability of assigning the current word token to each topic, conditioned on the topic assignments to all other word tokens $P(z_{i,j} = k \\mid \\mathbf{z}_{-i,j}, \\mathbf{w})$.\n",
    "\n",
    "$$\n",
    "P(z_{i,j} = k \\mid \\mathbf{z}_{-i,j}, \\mathbf{w}) \\propto\n",
    "\\frac{C^{WK}_{w_{i,j}, k} + \\beta}{\\sum^{W}_{w=1}C^{WK}_{w, k} + W \\beta}\n",
    "\\frac{C^{DK}_{d_{i}, k} + \\alpha}{\\sum^{K}_{k=1}C^{DK}_{d_{i}, k} + K \\alpha}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $W$ the number of words in the vocuabulary.\n",
    "* $D$ the number of documents in the corpus.\n",
    "* $K$ the number of topics.\n",
    "* $C^{WK}_{w_{i,j}, k}$ is the number of times word $w_{i,j}, k$ is assigned to topic $k$, not including the current instance ${(i,j)}$.\n",
    "* $C^{DT}_{d_{i}, k}$ is the number of times topic $k$ is assigned to some word in document $d_i$, not including current instance ${(i,j)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "word_prior = 0.3\n",
    "topic_prior = 0.01\n",
    "\n",
    "# Topic counts (number of observations)\n",
    "word_topic_obs = np.zeros(shape=(n_words_vocab, n_topics), dtype=int)\n",
    "doc_topic_obs = np.zeros(shape=(n_documents, n_topics), dtype=int)\n",
    "topic_obs = np.zeros(shape=n_topics, dtype=int)\n",
    "\n",
    "# Initial topic assignments for each word in each document\n",
    "documents_word_topics_sampled = [\n",
    "    [0 for _ in doc] for doc in documents_words]\n",
    "for i, doc in enumerate(documents_word_topics_sampled):\n",
    "    for j, word_topic in enumerate(doc):\n",
    "        # Initialise word topic randomly\n",
    "        k = np.random.randint(n_topics)\n",
    "        documents_word_topics_sampled[i][j] = k\n",
    "        w = documents_words[i][j]\n",
    "        # Count\n",
    "        word_topic_obs[w, k] += 1\n",
    "        doc_topic_obs[i, k] += 1\n",
    "        topic_obs[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs sampling estimation functions\n",
    "\n",
    "def estim_topic_word_dist(\n",
    "        word_idx, \n",
    "        word_topic_obs, \n",
    "        topic_obs, \n",
    "        word_prior, \n",
    "        n_words_vocab):\n",
    "    return (\n",
    "        (word_topic_obs[word_idx, :] + word_prior) / \n",
    "        (topic_obs + n_words_vocab * word_prior))\n",
    "\n",
    "def estim_doc_topic_dist(\n",
    "        doc_idx,\n",
    "        doc_topic_obs,\n",
    "        documents_words,\n",
    "        word_prior,\n",
    "        n_words_vocab):\n",
    "    return (\n",
    "        (doc_topic_obs[doc_idx, :] + topic_prior) / \n",
    "        (len(documents_words[doc_idx]) + n_topics * topic_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs sampling\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_iter = 2500\n",
    "for it in range(n_iter):\n",
    "    for i, doc in enumerate(documents_words):\n",
    "        for j, _ in enumerate(doc):\n",
    "            # Take current sample into account\n",
    "            w = documents_words[i][j]\n",
    "            k = documents_word_topics_sampled[i][j]\n",
    "            word_topic_obs[w, k] -= 1\n",
    "            doc_topic_obs[i, k] -= 1\n",
    "            topic_obs[k] -= 1\n",
    "            \n",
    "            # Calculate unnormalized conditional\n",
    "            p_w_given_k_estim = estim_topic_word_dist(\n",
    "                w, word_topic_obs, topic_obs, word_prior, n_words_vocab)\n",
    "            p_k_given_i_estim = estim_doc_topic_dist(\n",
    "                i, doc_topic_obs, documents_words, word_prior, n_words_vocab)\n",
    "            p_k_given_w_unnormalized = p_w_given_k_estim * p_k_given_i_estim\n",
    "            # Normalize the conditional\n",
    "            p_k_given_w = p_k_given_w_unnormalized / np.sum(p_k_given_w_unnormalized)\n",
    "\n",
    "            # Resample topic for given word and document\n",
    "            k = np.random.choice(a=n_topics, p=p_k_given_w)\n",
    "            # Update counts\n",
    "            documents_word_topics_sampled[i][j] = k\n",
    "            word_topic_obs[w, k] += 1\n",
    "            doc_topic_obs[i, k] += 1\n",
    "            topic_obs[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_assignment(\n",
    "    documents_words, \n",
    "    documents_word_topics_sampled,\n",
    "    'Inferred document topic assignement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer parameters\n",
    "\n",
    "doc_topic_dist_estim = np.zeros((n_documents, n_topics))\n",
    "for i in range(n_documents):\n",
    "    doc_topic_dist_estim[i] = estim_doc_topic_dist(\n",
    "        i, doc_topic_obs, documents_words, word_prior, n_words_vocab)\n",
    "\n",
    "topic_word_dist_estim = np.zeros((n_topics, n_words_vocab))\n",
    "for w in range(n_words_vocab):\n",
    "    topic_word_dist_estim[:,w] = estim_topic_word_dist(\n",
    "        w, word_topic_obs, topic_obs, word_prior, n_words_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regenerate from inferred parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "documents_words_gen = []\n",
    "documents_word_topics_gen = []\n",
    "for doc_idx in range(n_documents):\n",
    "    n_words_doc = int(1 + np.random.poisson(lam=4., size=1))\n",
    "    doc_topics = np.random.choice(\n",
    "        a=n_topics, p=doc_topic_dist_estim[doc_idx,:], size=n_words_doc)\n",
    "    doc_words = [\n",
    "        np.random.choice(\n",
    "            a=n_words_vocab, \n",
    "            p=topic_word_dist_estim[doc_topics[word_idx], :])\n",
    "        for word_idx in range(n_words_doc)]\n",
    "    documents_word_topics_gen.append(doc_topics.tolist())\n",
    "    documents_words_gen.append(doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_assignment(\n",
    "    documents_words_gen, \n",
    "    documents_word_topics_gen,\n",
    "    'Topic assigment of newly generated documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "TODO\n",
    "\n",
    "* [2.2 Variational Inference for LDA](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture15.pdf)\n",
    "* [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) (paper)\n",
    "* [A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation](https://papers.nips.cc/paper/3113-a-collapsed-variational-bayesian-inference-algorithm-for-latent-dirichlet-allocation.pdf)\n",
    "* [Inference Methods for Latent Dirichlet Allocation](http://times.cs.uiuc.edu/course/598f16/notes/lda-survey.pdf)\n",
    "* https://www.coursera.org/lecture/bayesian-methods-in-machine-learning/latent-dirichlet-allocation-ivnPr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
